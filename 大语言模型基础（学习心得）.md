# 大语言模型学习心得

# N-gram

N-gram 模型虽然简单有效，但有两个致命缺陷：**数据稀疏性 (Sparsity)**……**泛化能力差**：模型无法理解词与词之间的语义相似性。

它将词视为离散符号，无法处理未登录词（OOV），也无法捕捉“agent”和“robot”这类语义相近词的关联。这直接催生了**词嵌入（Word Embedding）** 的诞生，即用连续向量表示词，让模型具备了**语义空间中的泛化能力**，这是迈向现代深度学习语言模型的第一步。

# Transformer

Transformer在2017年由谷歌团队提出。它**完全抛弃了循环结构**，转而**完全依赖一种名为注意力 (Attention) 的机制**来捕捉序列内的依赖关系，从而实现了**真正意义上的并行计算**。

RNN/LSTM虽能处理长序列，但其**串行计算**的特性严重制约了训练效率和模型规模。Transformer通过**自注意力机制**，让序列中任意两个词元都能直接交互，并且所有位置可以**同时计算**，这为训练超大规模模型（如GPT、Llama）提供了**工程可行性**，是大语言模型时代开启的基石。

# **自注意力机制**

自注意力机制模拟了人类阅读时的聚焦行为。例如，在句子“The agent learns because it is intelligent.”中，模型能自动识别出“it”指代的是“agent”。通过**Query-Key-Value**的机制，模型动态地为每个词构建一个**富含全局上下文信息的新表示**。这是模型能够理解复杂句法和语义依赖的关键。

# **Decoder-Only**

**Decoder-Only**架构将所有任务（问答、写作、编程）都统一为“**自回归文本生成**”任务。这种极简的设计不仅**训练目标统一**，而且**结构简单、易于扩展**，成为构建通用智能体大脑的理想选择。

# 提示工程

**提示工程，就是研究如何设计出精准的提示，从而引导模型产生我们期望输出的回复。**

